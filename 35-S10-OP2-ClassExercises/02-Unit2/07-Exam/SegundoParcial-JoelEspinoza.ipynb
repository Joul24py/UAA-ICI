{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44829aea",
   "metadata": {},
   "source": [
    "### Universidad Autónoma de Aguascalientes\n",
    "\n",
    "### Centro de Ciencias Básicas\n",
    "\n",
    "### Departamento de Ciencias de la Computación\n",
    "\n",
    "### Optativa Profesionalizante II: Machine Learning y Deep Learning\n",
    "\n",
    "### 10° \"A\"\n",
    "\n",
    "### Segunda Evaluación Parcial\n",
    "\n",
    "### Docente: Dr. Francisco Javier Luna Rosas\n",
    "\n",
    "### Alumno: Joel Alejandro Espinoza Sánchez (211800)\n",
    "\n",
    "### Fecha de Entrega: Aguascalientes, Ags., 1 de mayo del 2023.\n",
    "\n",
    "---\n",
    "\n",
    "El análisis de sentimientos, a veces también denominado minería de opiniones, es una conocida sub-disciplina del amplio campo del PLN (Procesamiento del Lenguaje Natural); está relacionado con el análisis de la polaridad de documentos. Una tarea popular en el análisis de sentimiento es la clasificación de documentos basados en las emociones u opiniones expresadas de los autores respecto a un tema en particular. El conjunto de datos de críticas de cine consiste en 50,000 críticas de cine polarizadas etiquetadas como negativas y como positivas. Aquí, positiva significa que una película ha sido clasificada con más de seis estrellas, mientras que negativa significa que una película ha sido clasificada con menos de cinco estrellas.\n",
    "\n",
    "El alumno deberá elaborar un documento (```*.pdf```) y un archivo auto-reproducible (```*.html```) que analice, implemente y evalúe algoritmos de Deep Learning y Machine Learning para clasificar las críticas de cine. El documento deberá contener:\n",
    "\n",
    "- Portada\n",
    "\n",
    "- Evidencias del examen\n",
    "\n",
    "- Conclusiones\n",
    "\n",
    "- Referencias (formato APA)\n",
    "\n",
    "---\n",
    "\n",
    "### a) Una explicación del preprocesamiento de datos para generar un formato adecuado de los datos\n",
    "\n",
    "Primeramente se cargan los datos que se usarán:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad864182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"movie_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dec5f5a",
   "metadata": {},
   "source": [
    "Se realiza un preprocesamiento con la librería NLTK para limpiar el texto, tokenizar, normalizar, eliminar las stopwords y pos_tagging con el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d30edd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\alexe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "import re\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Eliminar caracteres especiales y números\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def normalize(tokens):\n",
    "    # Stemming\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stemmed_tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if not token in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "def pos_tagging(tokens):\n",
    "    pos_tokens = pos_tag(tokens)\n",
    "    return pos_tokens\n",
    "\n",
    "data['review'] = data['review'].apply(clean_text)\n",
    "data['review'] = data['review'].apply(tokenize)\n",
    "data['review'] = data['review'].apply(normalize)\n",
    "data['review'] = data['review'].apply(remove_stopwords)\n",
    "data['review'] = data['review'].apply(pos_tagging)\n",
    "\n",
    "X = np.asarray(data['review'])\n",
    "X = [' '.join([word[0] for word in sublist]) for sublist in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d120ff0a",
   "metadata": {},
   "source": [
    "### b) Una explicación del modelo bolsa de palabras o cualquier otro analizador ligústico aplicado al Dataset de críticas de cine\n",
    "\n",
    "El modelo de bolsa de palabras que se usó fue dado por TensorFlow de su apartado de Keras siendo el método ```Tokenizer``` de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a1819dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7294f5",
   "metadata": {},
   "source": [
    "### c) Una explicación de la transformación de las palabras en vectores de características (utilice la frecuencia de termino - frecuencia inversa de documento “tf-idf” o cualquier otra técnica que permita transformar palabras a vectores de características)\n",
    "\n",
    "La transformación de palabras en vectores de características se usa nuevamente TensorFlow con el uso de Keras usando ```pad_sequences``` de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ee9738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(X)\n",
    "X = pad_sequences(X, maxlen=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd2913a",
   "metadata": {},
   "source": [
    "### d) Una explicación del modelo CNN (Convoluciones 1D) para clasificar las críticas de cine. La precisión del modelo debe ser del 95% o mayor\n",
    "\n",
    "Explicación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38dce83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 0.4028 - accuracy: 0.8170 - val_loss: 0.3212 - val_accuracy: 0.8603\n",
      "Epoch 2/5\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 0.2602 - accuracy: 0.8928 - val_loss: 0.3084 - val_accuracy: 0.8675\n",
      "Epoch 3/5\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 0.1993 - accuracy: 0.9239 - val_loss: 0.3211 - val_accuracy: 0.8687\n",
      "Epoch 4/5\n",
      "1250/1250 [==============================] - 13s 11ms/step - loss: 0.1436 - accuracy: 0.9495 - val_loss: 0.3343 - val_accuracy: 0.8672\n",
      "Epoch 5/5\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 0.0912 - accuracy: 0.9722 - val_loss: 0.3796 - val_accuracy: 0.8658\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3796 - accuracy: 0.8658\n",
      "Loss:  0.3795654773712158\n",
      "Accuracy:  0.8658000230789185\n"
     ]
    }
   ],
   "source": [
    "y = np.array(data['sentiment'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, 32, input_length=200))\n",
    "model.add(Conv1D(32, 7, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d86c76",
   "metadata": {},
   "source": [
    "### e) Una explicación del análisis comparativo de modelos de Deep Learning (Redes Neuronales, Redes CNN (Convoluciones 1D)) Vs Machine Learning (KNN, Bayes, Arboles de Decisión, SVM, Random Forest, Potenciación, etc.), compare al menos dos clasificadores de cada uno con una precisión del 95% o mayor, el análisis deberá comparar: la precisión del modelo, el error del modelo, precisión negativa (especificidad), precisión positiva (sensibilidad), falsos positivos, falsos negativos, asertividad positiva, asertividad negativa\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15f35de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f643874a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusiones\n",
    "\n",
    "Es interesante e importante poder implementar las bases de estos temas para entenderlos en un futuro, pues, posteriormente no basta con sólo importar librerías que realicen el trabajo pesado, ya que, implementar manualmente estos algoritmos nos enseña a qué hay detrás del algoritmo, cómo funciona y poder comprender realmente qué está ocurriendo como la base de una red neuronal convolucional y la forma en la que ésta aprende. Es muy útil la implementación de estos algoritmos en estas tareas para las futuras tareas de la materia y aplicaciones de Machine Learning en la vida personal.\n",
    "\n",
    "## Referencias\n",
    "\n",
    "- Anónimo (s.f.) “Red neuronal artificial”. Obtenido de Wikipedia: https://es.wikipedia.org/wiki/Red_neuronal_artificial.\n",
    "- Data Scientest (2021) “Perceptrón. ¿Qué es y para qué sirve?”. Obtenido de Data Scientest: https://datascientest.com/es/perceptron-que-es-y-para-que-sirve.\n",
    "- Luna, F. (2023) “El Modelo de McCulloch – Pitts”. Apuntes de ICI 10°."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
